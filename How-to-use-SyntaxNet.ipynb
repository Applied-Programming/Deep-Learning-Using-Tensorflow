{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use SyntaxNet\n",
    "\n",
    "<hr>\n",
    "\n",
    "[SyntaxNet](https://github.com/tensorflow/models/tree/master/syntaxnet) is a neural-network Natural Language Processing framework for TensorFlow released by Google. It is one of the most powerful and accurate parsers. \n",
    "\n",
    "<h6>Courtesy: Google</h6>\n",
    "\n",
    "![Syntaxnet Demo](https://raw.githubusercontent.com/Aniruddha-Tapas/how-to-use-syntaxnet/master/syntaxnet.gif)\n",
    "\n",
    "\n",
    "Given a sentence as input, SyntaxNet tags each word with a part-of-speech (POS) tag that describes the word's syntactic function, and it determines the syntactic relationships between words in the sentence, represented in the dependency parse tree. These syntactic relationships are directly related to the underlying meaning of the sentence in question.\n",
    "\n",
    "## Building SyntaxNet\n",
    "\n",
    "There are two ways to actually build and run SyntaxNet:\n",
    "\n",
    "1. Building from source.\n",
    "2. Using [Docker](https://www.docker.com/).\n",
    "\n",
    "### 1. Building from source.\n",
    "\n",
    "You can build SyntaxNet by following the [nice guide](https://github.com/tensorflow/models/tree/master/syntaxnet#manual-installation) provided in the main SyntaxNet github branch. \n",
    "\n",
    "### 2. Using Docker.\n",
    "\n",
    "Building from source is doable but its computationally expensive and can result in few bugs while building if we mess up with the steps provided. Hence a clean way to build SyntaxNet is by using Docker. Here's what we are going to require :\n",
    "\n",
    "#### Prerequisites:\n",
    "\n",
    "* A 64-bit computer with at least 2 GB of RAM\n",
    "* The latest version of Docker\n",
    "* Ubuntu (I have only tested it on Ubuntu as of yet.)\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Pulling the Docker Image\n",
    "\n",
    "There could be other SyntaxNet images, but for demonstration, we'll be pulling an image created by [brianlow](https://github.com/brianlow/syntaxnet-docker).\n",
    "\t\n",
    "```docker pull brianlow/syntaxnet-docker```\n",
    "\n",
    "Depending on the speed of your network connection, you might have to wait for a while because the image is about 1GB.\n",
    "\n",
    "2. Once the SyntaxNet image is installed, you can now test it.  You need to create a new container using it and run a Bash shell on it.\n",
    "\n",
    "```docker run --name mcparseface --rm -i -t brianlow/syntaxnet-docker bash```\n",
    "\n",
    "3. Parsey McParseface, the pre-trained model that comes with SyntaxNet is powerful but slightly complicated. Thankfully, we can use the shell script `demo.sh` provided with SyntaxNet itself. All you need to do is pass an English sentence to it. \n",
    "\n",
    "```\n",
    "echo \"I found a website to post AI tutorials .\" syntaxnet/demo.sh\n",
    "```\n",
    "\n",
    "It generates the folling dependency parse tree as output:\n",
    "\n",
    "```\n",
    "Input: I found a website to post AI tutorials .\n",
    "Parse:\n",
    "found VBD ROOT\n",
    " +-- I PRP nsubj\n",
    " +-- website NN dobj\n",
    " |   +-- a DT det\n",
    " |   +-- post VB infmod\n",
    " |       +-- to TO aux\n",
    " |       +-- tutorials NNS dobj\n",
    " |           +-- AI NNP nn\n",
    " +-- . . punct\n",
    "```\n",
    "\n",
    "### Using SyntaxNet\n",
    "\n",
    "Instead of running the shell script, the code in this repo shows you how to use SyntaxNet. I have built a Python wrapper which calls the shell script and runs SyntaxNet. \n",
    "\n",
    "* Usage \n",
    "\n",
    "```python main.py '<some-english-text>.'```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main script to process the input string using nlp_util via the SyntaxNet parsing and finding root verb synonyms with accurate tenses using the Verb class,\n",
    "and then print the paraphrased sentence.\n",
    "\"\"\"\n",
    "import nlp_util\n",
    "\n",
    "#accept user input\n",
    "input_str = nlp_util.get_user_input()\n",
    "\n",
    "#return dependency tree\n",
    "dep_tree = nlp_util.create_dependency_tree()\n",
    "\n",
    "print \"Dependency tree:\"\n",
    "for d in dep_tree:\n",
    "\tprint d\n",
    "\n",
    "\n",
    "#retrieve root word and dependent object\n",
    "root = nlp_util.get_root_word(dep_tree)\n",
    "#print root\n",
    "\n",
    "dobj = nlp_util.get_dependent_object(dep_tree)\n",
    "#print dobj\n",
    "\n",
    "#retrieve synonym for root word\n",
    "synonym = nlp_util.get_synonym(root)\n",
    "#print synonym\n",
    "\n",
    "#display parahrased sentence\n",
    "print \"\\nParaphrased sentence :\\n\"\n",
    "print nlp_util.get_paraphrase(input_str,root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nlp_util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper class that provides NLP functionalities and calls the SyntaxNet shell script.\n",
    "\"\"\"\n",
    "\n",
    "import sys, os \n",
    "import subprocess\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import verb\n",
    "\n",
    "input_str = \"\"\n",
    "\n",
    "#accept user input\n",
    "def get_user_input():\n",
    "    input = sys.argv[1]\n",
    "    input_str = input\n",
    "    print(\"Input String: \" + input_str)\n",
    "    return input_str\n",
    "\n",
    "\n",
    "#return dependency tree\n",
    "def create_dependency_tree():\n",
    "    arg_list = sys.argv\n",
    "    arg_list.pop(0)\n",
    "    str1 = ' '.join(arg_list)\n",
    "    p = subprocess.Popen(\"echo \" + str1 + \"| sudo docker run --rm -i brianlow/syntaxnet-docker\", stdout=subprocess.PIPE, shell=True)\n",
    "    out = p.stdout.read()\n",
    "    deptree = out.splitlines()\n",
    "    return deptree\n",
    "\n",
    "#retrieve root word \n",
    "def get_root_word(dependency_tree):\n",
    "    root = dependency_tree[2].split()\n",
    "    return root\n",
    "\n",
    "#retrieve dependent object\n",
    "def get_dependent_object(dependency_tree):\n",
    "    #print \"Getting dependency tree\"\n",
    "    #for d in dependency_tree:\n",
    "    #\tprint d\n",
    "    for string in dependency_tree:\n",
    "        if string.find(\"dobj\") != -1:\n",
    "            dobj = string.split()[1]\n",
    "            return dobj\n",
    "\n",
    "#retrieve synonym for root word\n",
    "def get_synonym(root):\n",
    "\n",
    "    listofsyns = wordnet.synsets(root[0])\n",
    "    synonym = listofsyns[3].name().split(\".\")[0]\n",
    "\n",
    "    if root[1] =='VBD':\n",
    "        synonym = verb.verb_past(synonym)\n",
    "    elif root[1] =='VBG':\n",
    "        synonym = verb.verb_present_participle(synonym)\n",
    "    elif root[1] =='VBN':\n",
    "        synonym = verb.verb_past_participle(synonym)\n",
    "    elif root[1] =='VBP':\n",
    "        synonym = verb.verb_present(synonym, person=3, negate=True)\n",
    "    elif root[1] =='VBZ':\n",
    "        synonym = verb.verb_present(synonym, person=3, negate=False)\n",
    "\n",
    "    return synonym\n",
    "\n",
    "#retrieve paraphrased sentence\n",
    "def get_paraphrase(input_str,root):\n",
    "    list_str = input_str.split()\n",
    "    stop = set(stopwords.words('english'))\n",
    "    paraphrase = []\n",
    "    for word in list_str:\n",
    "        try:\n",
    "            if word == root[0]:\n",
    "                if word.lower() not in stop:\n",
    "                    paraphrase.append(get_synonym(root))\t\n",
    "            else:\n",
    "                paraphrase.append(word) \n",
    "        except Exception: \n",
    "            pass\n",
    "    paraphrased_str = \" \".join(paraphrase)\n",
    "    return paraphrased_str\n",
    "    #return paraphrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output :\n",
    "\n",
    "```\n",
    "Dependency tree:\n",
    "Input: I found a website to post AI tutorials .\n",
    "Parse:\n",
    "found VBD ROOT\n",
    " +-- I PRP nsubj\n",
    " +-- website NN dobj\n",
    " |   +-- a DT det\n",
    " |   +-- post VB infmod\n",
    " |       +-- to TO aux\n",
    " |       +-- tutorials NNS dobj\n",
    " |           +-- AI NNP nn\n",
    " +-- . . punct\n",
    " \n",
    "Paraphrased sentence : \n",
    "I established a website to post AI tutorials.\n",
    "```\n",
    "\n",
    "\n",
    "Basically what this script does is:\n",
    "* it parses the input sentence\n",
    "* finds the root verb from the sentence using the dependency tree created by SyntaxNet\n",
    "* finds a synonym of the root verb using the nltk library\n",
    "* replaces the original verb with the synonym\n",
    "\n",
    "You'll have to run `pip install nltk` to install nltk. The `Verb` folder contains the code to find out the tense of the root verb so that its synonym can be coded to be the same tense, in order to keep the meaning of the sentence intact.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Thus this code is a pretty simple approach to use SyntaxNet for basic paraphrasing. It's just a demo and it doesn't always find the paraphrase and hence in this case returns the same sentence. But our main goal is to run SyntaxNet and leverage its capabilities to find the proper context in a given text and use it to find important pieces of text like the `Root Word`, `Dependent Object` etc.\n",
    "\n",
    "* To learn more about all the Part of Speech Tags that appear in the SyntaxNet output, please visit [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
    "\n",
    "* And to understand the dependencies that SyntaxNet tries to find within a sentence, please visit [here](http://universaldependencies.org/en/dep/).\n",
    "\n",
    "These links would definitely help you to understand the output of SyntaxNet better. Feel free to improvise!\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
